{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Importing Necessary Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "from stdnum import py\n",
    "from pygments.lexers import go\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Importing Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape -  (569, 33)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r'dataset_breast_cancer.csv')\n",
    "\n",
    "# show the data of how many rows and columns\n",
    "print(\"Data Shape - \", data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Read the Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 33 columns</p>\n</div>"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# description of the current data set\n",
    "data.describe(include='all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the first look in the data description we can see that :\n",
    "    - B = benign is the most frequent value in our target columns\n",
    "    - Unnamed: 32nd columns is an empty column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to the data, all features are numerical values except the target value diagnosis which is an object: M = malignant, B = benign."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. EDA (Exploratory Data Analysis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Drop Out Unnecessary Column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the 32nd colum is empty, we will drop it out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 32',axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Check the Missing Value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "percent_missing = data.isnull().sum()/data.shape[0]*100\n",
    "\n",
    "value = {\n",
    "    'missing_values ':missing_values,\n",
    "    'percent_missing %':percent_missing\n",
    "}\n",
    "frame=pd.DataFrame(value)\n",
    "frame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the tables, it shows that the data is cleaned and no missing value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transformation of type of the target value to numerical\n",
    "le = preprocessing.LabelEncoder()\n",
    "data.diagnosis = le.fit_transform(data.diagnosis)\n",
    "data.diagnosis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Diagnosis:\n",
    "\n",
    "M = malignant => 1\n",
    "B = benign => 0\n",
    "\n",
    "Let's also drop out an id column since we also don't need it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop the id columns\n",
    "data = data.drop('id',axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Correlation Matrix with Heatmap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A graphical representation of a correlation matrix representing the correlation between different variables. The value of correlation can take any value\n",
    "from -1 to 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#independent columns\n",
    "X = data.iloc[:, 0:20]\n",
    "#target column\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "#get correlations of each features in dataset\n",
    "corrmatrix = data.corr()\n",
    "top_corr_features = corrmatrix.index\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "#plot heat map\n",
    "g = sns.heatmap(data[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above correlation heatmap, we could get some of the following information:\n",
    "\n",
    "- Variables such as radius_worst & radious_mean, radius_worst & parameter_mean, are having strong positive correlation, just to name a few.\n",
    "- Variables such as radius_worst & smoothness_se, and fractal_dimension_mean & radious_mean are having strong negative correlations: also, just to name a few.\n",
    "- overall, to view this correlation graph: there are several variables that have no correlation and whose correlation value is near 0, while whose that have strong correlation is closer to 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 Positive Correlated Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# B = benign => 0\n",
    "# M = malignant => 1\n",
    "palette ={0 : 'lightblue', 1 : 'gold'}\n",
    "edgecolor = 'grey'\n",
    "\n",
    "# Plot +\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.subplot(221)\n",
    "ax1 = sns.scatterplot(x = data['perimeter_mean'], y = data['radius_worst'], hue = \"diagnosis\",\n",
    "                    data = data, palette = palette, edgecolor=edgecolor)\n",
    "plt.title('perimeter mean vs radius worst')\n",
    "plt.subplot(222)\n",
    "ax2 = sns.scatterplot(x = data['area_mean'], y = data['radius_worst'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('area mean vs radius worst')\n",
    "plt.subplot(223)\n",
    "ax3 = sns.scatterplot(x = data['texture_mean'], y = data['texture_worst'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('texture mean vs texture worst')\n",
    "plt.subplot(224)\n",
    "ax4 = sns.scatterplot(x = data['area_worst'], y = data['radius_worst'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('area mean vs radius worst')\n",
    "\n",
    "fig.suptitle('Positive correlated features', fontsize = 20)\n",
    "plt.savefig('1')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 Uncorrelated Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# B = benign => 0\n",
    "# M = malignant => 1\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.subplot(221)\n",
    "ax1 = sns.scatterplot(x = data['smoothness_mean'], y = data['texture_mean'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('smoothness mean vs texture mean')\n",
    "plt.subplot(222)\n",
    "ax2 = sns.scatterplot(x = data['radius_mean'], y = data['fractal_dimension_worst'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('radius mean vs fractal dimension_worst')\n",
    "plt.subplot(223)\n",
    "ax3 = sns.scatterplot(x = data['texture_mean'], y = data['symmetry_mean'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('texture mean vs symmetry mean')\n",
    "plt.subplot(224)\n",
    "ax4 = sns.scatterplot(x = data['texture_mean'], y = data['symmetry_se'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('texture mean vs symmetry se')\n",
    "\n",
    "fig.suptitle('Uncorrelated features', fontsize = 20)\n",
    "plt.savefig('2')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6 Negative Correlated Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# B = benign => 0\n",
    "# M = malignant => 1\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.subplot(221)\n",
    "ax1 = sns.scatterplot(x = data['area_mean'], y = data['fractal_dimension_mean'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('smoothness mean vs fractal dimension mean')\n",
    "plt.subplot(222)\n",
    "ax2 = sns.scatterplot(x = data['radius_mean'], y = data['fractal_dimension_mean'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('radius mean vs fractal dimension mean')\n",
    "plt.subplot(223)\n",
    "ax2 = sns.scatterplot(x = data['area_mean'], y = data['smoothness_se'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('area mean vs fractal smoothness se')\n",
    "plt.subplot(224)\n",
    "ax2 = sns.scatterplot(x = data['smoothness_se'], y = data['perimeter_mean'], hue = \"diagnosis\",\n",
    "                    data = data, palette =palette, edgecolor=edgecolor)\n",
    "plt.title('smoothness se vs perimeter mean')\n",
    "\n",
    "fig.suptitle('Negative correlated features', fontsize = 20)\n",
    "plt.savefig('3')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Data Vizualisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Diagnosis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#bar chart\n",
    "plt.rcParams['figure.figsize']=7,7\n",
    "sns.set_style(\"darkgrid\")\n",
    "ax = sns.countplot(x=data.diagnosis , palette = \"rocket\", saturation =1.5)\n",
    "plt.xlabel(\"diagnosis malignant = 1 / benign = 0 \", fontsize = 15 )\n",
    "plt.ylabel(\"count\", fontsize = 20)\n",
    "plt.title('Number of diagnosis ', fontsize = 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pie chart in percentile\n",
    "# B = benign => 0\n",
    "# M = malignant => 1\n",
    "label=data.diagnosis.value_counts().index\n",
    "count=data.diagnosis.value_counts().values\n",
    "color = ['orange', '#8B5A8C']\n",
    "\n",
    "plt.pie(count,labels=label)\n",
    "plt.title('Distribution of diagnosis variable', fontsize = 20)\n",
    "plt.figure(1, figsize=(20,15))\n",
    "plt.pie(count, labels=label, colors=color, autopct='%1.1f%%')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ploting the histogram of these values so we can better observer their values and data distribution. In order to do so, we are going to separate, for each histogram, the values depending on the diagnosis column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Features vs Diagnosis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n",
    "- mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# B = benign => 0\n",
    "# M = malignant => 1\n",
    "\n",
    "features_mean=list(data.columns[1:11])\n",
    "# split dataframe into two based on diagnosis\n",
    "dfM=data[data['diagnosis'] ==1]\n",
    "dfB=data[data['diagnosis'] ==0]\n",
    "\n",
    "#Stack the data\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,12))\n",
    "axes = axes.ravel()\n",
    "for idx,ax in enumerate(axes):\n",
    "    ax.figure\n",
    "    binwidth= (max(data[features_mean[idx]]) - min(data[features_mean[idx]]))/50\n",
    "    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(data[features_mean[idx]]), max(data[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, density = True, label=['Malignant','Benign'], color=['r','g'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(features_mean[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From these ten graphs we can observe, these features might be useful in predicting whether a patient has cancer or not due to the distinct grouping between malignant and benign. We can also see the most frequent malignant value of these features under each graph, although visually they may vary since we might not have an optimal bin width, bust most of them match with the values histograms show."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Feature Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# B = benign => 0\n",
    "# M = malignant => 1\n",
    "\n",
    "plt.rcParams['figure.figsize']=16,7\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "x = data.drop('diagnosis',axis=1)\n",
    "y = data.diagnosis\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x,y)\n",
    "\n",
    "print(model.feature_importances_)\n",
    "feat_importance = pd.Series(model.feature_importances_, index=x.columns)\n",
    "\n",
    "feat_importance.nlargest(15).plot(kind='barh', fontsize=12)\n",
    "plt.title('the 15th most important features are', fontsize=15)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check our current column name."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop out unimportant features, and choose just the best 15 according the bar chart."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=data.drop(['texture_mean','smoothness_mean','compactness_mean','symmetry_mean','perimeter_se','compactness_se','concavity_se','concave points_se','smoothness_worst','symmetry_worst','fractal_dimension_worst', 'fractal_dimension_mean','texture_se','smoothness_se','symmetry_se','fractal_dimension_se'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we manipulate the data to prepare it for modeling. There are three main steps that we can take:\n",
    "\n",
    "- Splitting the data into a training set, a validation set (to help me develop my models), and a test set (to help me evaluate the final version of each model);\n",
    "- Resampling the training set so that all classes are equally represented;\n",
    "- Scaling the data, which will help ensure that PCA and some machine learning algorithms work properly;\n",
    "- Principal Components Analysis (PCA), which will reduce the dimensions of the data and eliminate any multicollinearity.\n",
    "\n",
    "We do all these steps first to the training data so that we can check the outcome at each step. Once that's done, we put the essential preprocessing steps into a pipeline that I can use to transform the validation and test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Train-Validation-Test Split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will take 80% of the total dataset to use as training data. The remaining 20% of the original dataset will be devoted half of that to validation and half to be used as a true holdout set, which will be used to evaluate the final versions of each of my models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split first into training and test datasets\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "y = data.diagnosis\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split again into validation and true holdout (test) datasets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5,\n",
    "                                                random_state=1,\n",
    "                                                stratify=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Examine shapes of the subsets\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we will compare model performance on the raw data and the preprocessed version, so at this point (before preprocessing) we will save copies for later used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save raw copies of train and validation sets before further preprocessing\n",
    "X_train_raw = X_train.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "\n",
    "X_val_raw = X_val.copy()\n",
    "y_val_raw = y_val.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Resampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The preprocessing step is to deal with the class imbalance. As we can see earlier Benign: 0 is more than 60% pf the data set while Malignant:1 is only 30%\n",
    "The overall strategy is to under-sample the bigger classes and over-sample the smaller ones so that both classes are the same size as the median-sized class.\n",
    "\n",
    "First, Let's put the training data back into one DataFrame to make things a little easier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n"
     ]
    },
    {
     "data": {
      "text/plain": "     radius_mean  perimeter_mean  area_mean  concavity_mean  \\\n195        12.91           82.53      516.4         0.03873   \n560        14.05           91.38      600.4         0.04462   \n544        13.87           89.77      584.8         0.03688   \n495        14.87           96.12      680.9         0.06824   \n527        12.34           78.94      468.5         0.02958   \n\n     concave points_mean  radius_se  area_se  radius_worst  texture_worst  \\\n195              0.02377     0.1942   15.750         13.88          22.00   \n560              0.04304     0.3645   29.840         15.30          33.17   \n544              0.02369     0.2720   23.120         15.05          24.75   \n495              0.04951     0.2323   21.840         16.01          28.48   \n527              0.02647     0.1166    8.955         13.61          19.27   \n\n     perimeter_worst  area_worst  compactness_worst  concavity_worst  \\\n195            90.81       600.6             0.1506           0.1764   \n560           100.20       706.7             0.2264           0.1326   \n544            99.17       688.6             0.2037           0.1377   \n495           103.90       783.6             0.1388           0.1700   \n527            87.22       564.9             0.2074           0.1791   \n\n     concave points_worst  diagnosis  \n195               0.08235          0  \n560               0.10480          0  \n544               0.06845          0  \n495               0.10170          0  \n527               0.10700          0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>radius_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>radius_se</th>\n      <th>area_se</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>195</th>\n      <td>12.91</td>\n      <td>82.53</td>\n      <td>516.4</td>\n      <td>0.03873</td>\n      <td>0.02377</td>\n      <td>0.1942</td>\n      <td>15.750</td>\n      <td>13.88</td>\n      <td>22.00</td>\n      <td>90.81</td>\n      <td>600.6</td>\n      <td>0.1506</td>\n      <td>0.1764</td>\n      <td>0.08235</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>560</th>\n      <td>14.05</td>\n      <td>91.38</td>\n      <td>600.4</td>\n      <td>0.04462</td>\n      <td>0.04304</td>\n      <td>0.3645</td>\n      <td>29.840</td>\n      <td>15.30</td>\n      <td>33.17</td>\n      <td>100.20</td>\n      <td>706.7</td>\n      <td>0.2264</td>\n      <td>0.1326</td>\n      <td>0.10480</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>544</th>\n      <td>13.87</td>\n      <td>89.77</td>\n      <td>584.8</td>\n      <td>0.03688</td>\n      <td>0.02369</td>\n      <td>0.2720</td>\n      <td>23.120</td>\n      <td>15.05</td>\n      <td>24.75</td>\n      <td>99.17</td>\n      <td>688.6</td>\n      <td>0.2037</td>\n      <td>0.1377</td>\n      <td>0.06845</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>14.87</td>\n      <td>96.12</td>\n      <td>680.9</td>\n      <td>0.06824</td>\n      <td>0.04951</td>\n      <td>0.2323</td>\n      <td>21.840</td>\n      <td>16.01</td>\n      <td>28.48</td>\n      <td>103.90</td>\n      <td>783.6</td>\n      <td>0.1388</td>\n      <td>0.1700</td>\n      <td>0.10170</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>12.34</td>\n      <td>78.94</td>\n      <td>468.5</td>\n      <td>0.02958</td>\n      <td>0.02647</td>\n      <td>0.1166</td>\n      <td>8.955</td>\n      <td>13.61</td>\n      <td>19.27</td>\n      <td>87.22</td>\n      <td>564.9</td>\n      <td>0.2074</td>\n      <td>0.1791</td>\n      <td>0.10700</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate X_train and y_train for resampling\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "print(len(df_train))\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "0    285\n1    170\nName: diagnosis, dtype: int64"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for class imbalance\n",
    "df_train.diagnosis.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use RandomUnderSampler and SMOTE to undersample the larger classes and oversample the smaller one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\alici\\AppData\\Local\\Temp\\ipykernel_18488\\198468957.py\", line 1, in <cell line: 1>\n",
      "    from imblearn.under_sampling import RandomUnderSampler\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\imblearn\\__init__.py\", line 52, in <module>\n",
      "    from . import combine\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\imblearn\\combine\\__init__.py\", line 5, in <module>\n",
      "    from ._smote_enn import SMOTEENN\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py\", line 11, in <module>\n",
      "    from ..over_sampling import SMOTE\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\imblearn\\over_sampling\\__init__.py\", line 8, in <module>\n",
      "    from ._smote import SMOTE\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\__init__.py\", line 5, in <module>\n",
      "    from .cluster import KMeansSMOTE\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\cluster.py\", line 14, in <module>\n",
      "    from sklearn.cluster import MiniBatchKMeans\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\sklearn\\cluster\\__init__.py\", line 6, in <module>\n",
      "    from ._spectral import spectral_clustering, SpectralClustering\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\sklearn\\cluster\\_spectral.py\", line 21, in <module>\n",
      "    from ..manifold import spectral_embedding\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\sklearn\\manifold\\__init__.py\", line 5, in <module>\n",
      "    from ._locally_linear import locally_linear_embedding, LocallyLinearEmbedding\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\sklearn\\manifold\\_locally_linear.py\", line 12, in <module>\n",
      "    from ..base import (\n",
      "ImportError: cannot import name '_ClassNamePrefixFeaturesOutMixin' from 'sklearn.base' (c:\\languages\\python\\python390\\lib\\site-packages\\sklearn\\base.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 845, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, \"bg:ansiyellow\")\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\stack_data\\core.py\", line 424, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    elif 'noinherit' in styledefs and token is not Token:\n",
      "  File \"c:\\languages\\python\\python390\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    class StyleMeta(type):\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Randomly undersample the larger classes\n",
    "rus = RandomUnderSampler(random_state=2,\n",
    "                         sampling_strategy={0:170, 1:170,})\n",
    "\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class counts\n",
    "Counter(y_rus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Randomly oversample the smaller classes\n",
    "smote = SMOTE(random_state=3, sampling_strategy={4:16408, 5:16408, 6:16408})\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_rus, y_rus)\n",
    "\n",
    "# Check class counts\n",
    "Counter(y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Define Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part is essential to measure the performance of a model : roc, cross validation, learning curve."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.1. Confusion Matrix and Show Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The confusion matrix, also known as the error matrix, allows visualization of the performance of an algorithm :\n",
    "\n",
    "true positive (TP) : Malignant tumour correctly identified as malignant\n",
    "true negative (TN) : Benign tumour correctly identified as benign\n",
    "false positive (FP) : Benign tumour incorrectly identified as malignant\n",
    "false negative (FN) : Malignant tumour incorrectly identified as benign\n",
    "\n",
    "Metrics :\n",
    "\n",
    "Accuracy : (TP +TN) / (TP + TN + FP +FN)\n",
    "Precision : TP / (TP + FP)\n",
    "Recall : TP / (TP + FN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Confusion matrix\"',\n",
    "                          cmap = plt.cm.Blues) :\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment = 'center',\n",
    "                 color = 'white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show metrics\n",
    "cm = confusion_matrix(y_test, y_score)\n",
    "\n",
    "def show_metrics():\n",
    "    tp = cm[1,1]\n",
    "    fn = cm[1,0]\n",
    "    fp = cm[0,1]\n",
    "    tn = cm[0,0]\n",
    "    #print('Accuracy  =     {:.3f}'.format((tp+tn)/(tp+tn+fp+fn)))\n",
    "    print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n",
    "    print('Recall    =     {:.3f}'.format(tp/(tp+fn)))\n",
    "    print('F1_score  =     {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/\n",
    "                                                 ((tp/(tp+fp))+(tp/(tp+fn))))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Precision â€“ Recall curve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The precision-recall curve shows the tradeoff between precision and recall for different threshold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Precision-recall curve\n",
    "def plot_precision_recall():\n",
    "    plt.step(recall, precision, color = 'b', alpha = 0.2,\n",
    "             where = 'post')\n",
    "    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n",
    "                 color = 'b')\n",
    "\n",
    "    plt.plot(recall, precision, linewidth=2)\n",
    "    plt.xlim([0.0,1])\n",
    "    plt.ylim([0.0,1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.3. ROC curveÂ¶"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "def plot_roc():\n",
    "    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n",
    "    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n",
    "   # plt.xlim([0.0,0.001])\n",
    "   # plt.ylim([0.0,1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.4  Learning curve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Learning curve determines cross-validated training and test scores."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n",
    "                        n_jobs = 1, train_sizes = np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv = cv, n_jobs = n_jobs, train_sizes = train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis = 1)\n",
    "    train_scores_std = np.std(train_scores, axis = 1)\n",
    "    test_scores_mean = np.mean(test_scores, axis = 1)\n",
    "    test_scores_std = np.std(test_scores, axis = 1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha = 0.1, color = \"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color = \"r\",\n",
    "             label = \"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color = \"g\",\n",
    "             label = \"Cross-validation score\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    return plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.5 Cross validation metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cross val metric\n",
    "def cross_val_metrics(model) :\n",
    "    scores = ['accuracy', 'precision', 'recall']\n",
    "    for sc in scores:\n",
    "        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n",
    "        print('[%s] : %0.5f (+/- %0.5f)'%(sc, scores.mean(), scores.std()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Machine Learning Applications"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "y = diagnosis (target)\n",
    "X = features (radius_mean, area_se, ....)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Def X and y\n",
    "y = np.array(data.diagnosis.tolist())\n",
    "data = data.drop('diagnosis', 1)\n",
    "X = np.array(data.as_matrix())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standard scaler (X) to help to rescale the attributes so that they have mean as 0 and variance as 1.\n",
    "The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train_test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.12, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split again into validation and true holdout (test) datasets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5,\n",
    "                                                random_state=1,\n",
    "                                                stratify=y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Examine shapes of the subsets\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#findout best parameter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#fit the models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualisation of the result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualie the result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Hyperparameter tunning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualise the result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Displaying Best Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n",
    "              'Random Forest', 'Decision Tree', 'LightGBM', 'GBM', 'GBM2', 'AdaBoost',\n",
    "              'XGBoost', 'CatBoost', 'Naive Bayes'],\n",
    "    'Score': [acc_svm, acc_knn, acc_logreg, acc_randomforest, acc_dt, acc_lgb,\n",
    "              acc_gbm, acc_gbm2, acc_adaboost, acc_xgboost, acc_catboost, acc_nb]})\n",
    "models.sort_values(by='Score', ascending=False)\n",
    "plt.rcParams['figure.figsize'] = 15, 6\n",
    "sns.set_style(\"darkgrid\")\n",
    "ax = sns.barplot(x=models.Model, y=models.Score, palette=\"rocket\", saturation=1.5)\n",
    "plt.xlabel(\"Classifier Models\", fontsize=20)\n",
    "plt.ylabel(\"% of Accuracy\", fontsize=20)\n",
    "plt.title(\"Accuracy of different Classifier Models\", fontsize=20)\n",
    "plt.xticks(fontsize=12, horizontalalignment='center', rotation=8)\n",
    "plt.yticks(fontsize=13)\n",
    "for p in ax.patches:\n",
    "    width, height = p.get_width(), p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height:.2%}', (x + width / 2, y + height * 1.02), ha='center', fontsize='x-large')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}